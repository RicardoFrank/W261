{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #1=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #1  (version 2016-01-14)\n",
    "\n",
    "---------------\n",
    "=== INSTRUCTIONS for SUBMISSIONS ===\n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "\n",
    "=== Week 1 ASSIGNMENTS ===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define big data. Provide an example of a big data problem in your domain of expertise. **\n",
    "\n",
    "Big Data is a problem space / solution space that requires a different computation, storage, and operation paradigm than previously required due to certain constraints, mainly:\n",
    "\n",
    "1. Scale\n",
    "2. Hardware capabilities\n",
    "3. Price\n",
    "\n",
    "Customers rely upon Big Data solutions when their data becomes too large to manage and / or process in a single machine, given their business constraints:\n",
    "\n",
    "1. Velocity\n",
    "2. Volume\n",
    "3. Variety\n",
    "\n",
    "I work on the Big Data team at Microsoft, specifically Azure Data Lake, and we see many different problems and scenarios.  The most clear example is building the Bing Index on a Big Data solution due to scalability, freshness, and cost constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?**\n",
    "\n",
    "I would estimate bias and variance using the bias variance equations\n",
    "\n",
    "**Bias: $E[h(x*)] - f(x*)$**\n",
    "\n",
    "Meaning, (avg model prediction - observed value)\n",
    "\n",
    "**Variance: $E[ ( h(x*) - E[h(x*)] ) ^ 2 ]$**\n",
    "\n",
    "Meaning avg ( square error of (model prediction - avg of all models predictions) )\n",
    "\n",
    "So, for each polynomial model, I would run multiple iterations with bootstrap samples over the test data set (e.g. 20 iterations per polynomial order).  This would provide a set of regression lines\n",
    "\n",
    "$M_p$, set of regression lines for polynomial order p\n",
    "\n",
    "Given $M_p$ for $p$ $\\epsilon$ $[1,5]$, I compute bias and variance over $M_p$ and the domain values for $x$.  \n",
    "\n",
    "I would then select the best model using\n",
    "\n",
    "$E = bias^2 + variance$\n",
    "\n",
    "This is because E captures the bias and variance tradeoff best and we want to minimize bias and variance.  From $M_p$, I now have $M_p^{best}$.\n",
    "\n",
    "Estimating the noise is trickier though, and I would make the assumption that the data is polynomial in nature so any difference between observation and the optimal model is noise.  If not true, then I've chosen the wrong model basis to begin with which is a wholly different problem.\n",
    "\n",
    "Noise:\n",
    "\n",
    "$E[ (y* - f(x*))^2 ]$\n",
    "avg square error between observed and actual value, which we assume is perfectly represented by $M_p^{best}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remainder of this assignment you will produce a spam filter\n",
    "that is backed by a multinomial naive Bayes classifier b (see http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html),\n",
    "which counts words in parallel via \n",
    "a unix, poor-man's map-reduce framework.\n",
    "\n",
    "For the sake of this assignment\n",
    "we will focus on the basic construction \n",
    "of the parallelized classifier,\n",
    "and not consider its validation or calibration,\n",
    "and so you will have the classifier operate\n",
    "on its own training data (unlike a field application where one would use non-overlapping subsets for training, validation and testing).\n",
    "\n",
    "The data you will use is a curated subset of the Enron email corpus\n",
    "(whose details you may find in the file enronemail_README.txt \n",
    "in the directory surrounding these instructions).\n",
    "\n",
    "NOTE: please use the subject field and the body field for all your Naive Bayes modeling. \n",
    "\n",
    "NOTE: This SPAM/HAM dataset for HW1 contains 100 records from the Enron SPAM/HAM corpus. Please limit your study to this. There are about 93,000 emails in the original SPAM/HAM corpus. There are several versions of the SPAM/HAM corpus. Other Enron-Spam datasets are available from http://www.iit.demokritos.gr/skel/i-config/ and http://www.aueb.gr/users/ion/publications.html in both raw and pre-processed form. \n",
    "\n",
    "\n",
    "=====Instructions/Goals=====\n",
    "\n",
    "In this directory you will also find starter code (pNaiveBayes.sh),\n",
    "(similar to the pGrepCount.sh code that was presented in this weeks lectures),\n",
    "which will be used as control script to a python mapper and reducer \n",
    "that you will supply at several stages. Doing some exploratory data analysis you will see (with this very small dataset) the following\\:\n",
    "```\n",
    "> wc -l enronemail_1h.txt  #100 email records\n",
    "     100 enronemail_1h.txt\n",
    "> cut -f2 -d$'\\t' enronemail_1h.txt|wc  #extract second field which is SPAM flag\n",
    "     101     394    3999\n",
    "\n",
    "JAMES-SHANAHANs-Desktop-Pro-2:HW1-Questions jshanahan$ cut -f2 -d$'\\t' enronemail_1h.txt|head\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "1\n",
    "\n",
    "> head -n 100 enronemail_1h.txt|tail -1|less #an example SPAM email record\n",
    "018.2001-07-13.SA_and_HP       1        [ilug] we need your assistance to invest in your country        dear sir/madam,  i am well confident of your capability to assist me in  a transaction for mutual benefit of both parties, ie  (me and you) i am also believing that you will not  expose or betray the trust and confidence i am about  to establish with you. i have decided to contact you  with greatest delight and personal respect.  well, i am victor sankoh, son to mr. foday  sankoh  who was arrested by the ecomog peace keeping force  months ago in my country sierra leone. ….\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "print \"done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.**\n",
    "\n",
    "   To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word.\n",
    "\n",
    "CROSSCHECK: \n",
    "```\n",
    ">grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l\n",
    "       8   \n",
    "```       \n",
    "#\"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!\n",
    "\n",
    "\n",
    "See example in: http://nbviewer.ipython.org/urls/dl.dropbox.com/s/ujz9w7d2a73b80o/DivideAndConquer2-python-Incomplete.ipynb\n",
    "See video section 1.12.1 1.12.1 Poor Man's MapReduce Using Command Line (Part 2) located at: \n",
    "https://learn.datascience.berkeley.edu/mod/page/view.php?id=10961\n",
    "NOTE in your python notebook create a cell to save your mapper/reducer to disk using magic commands (see example here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "#findwords = re.split(\" \",sys.argv[2].lower())\n",
    "findword = sys.argv[2]\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    input_file = myfile.read()\n",
    "    print re.findall(findword, input_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "filelist = re.split(\" \",sys.argv[1].lower())\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for file_chunk in filelist:\n",
    "    with open (file_chunk, \"r\") as myfile:\n",
    "        input_file = myfile.read()\n",
    "        sum += len(input_file.split())\n",
    "\n",
    "print sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the next cell use the following to change the permissions of the mapper/reducer using the following commands.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that**\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "\n",
    "   that performs a single word Naive Bayes classification. For multinomial Naive Bayes, the $Pr(X=“assistance”|Y=SPAM)$ is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   NOTE if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeld as SPAM (when concatenated) is 1,000. Then $Pr(X=“assistance”|Y=SPAM) = 5/1000$. Note this is a multinomial estimated of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "##The following mapper reducer pair does organization in the mapper.\n",
    "##The output is a list with entries being (ID, target_word_count), and the last entry being a list of words in spam\n",
    "##The reducer takes the list of all words in all spam, creates a total list\n",
    "##Using the total list size and target_word_count, we compute the probability of spam\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]  #ok for this example since we only have 1 arg\n",
    " \n",
    "lines = open(filename).read().splitlines()\n",
    "\n",
    "#no need to check lines for correct number of fields.  All have 4\n",
    "\n",
    "set_of_words_spam = \"\"\n",
    "set_of_words_not_spam = \"\"\n",
    "output_set = []\n",
    "spam_count = 0\n",
    "email_count = 0\n",
    "target_in_spam_count = 0\n",
    "target_in_not_spam_count = 0\n",
    "words_in_spam_count = 0\n",
    "words_in_not_spam_count = 0\n",
    "\n",
    "#append ID and target word count to output\n",
    "#get set of words in spam\n",
    "#get [spam email, email] count\n",
    "for line in lines:\n",
    "    if len(line.split('\\t')) == 4:\n",
    "        tmp = line.split('\\t')\n",
    "        all_text = tmp[2] + \" \" + tmp[3]\n",
    "        \n",
    "        #append the tuple (ID, target word count)\n",
    "        target_word_count = len(re.findall(findword,all_text))\n",
    "        output_set.append(str(tmp[0] + \",\" + str(target_word_count)))\n",
    "            \n",
    "        #if spam\n",
    "        if tmp[1] == \"1\":\n",
    "            #check to see if we've seen the word before in spam\n",
    "            for word in all_text.split(' '):\n",
    "                if word not in set_of_words_spam:\n",
    "                    set_of_words_spam = set_of_words_spam + word + \" \"\n",
    "            \n",
    "            #increment number of times seen in spam and total spam words\n",
    "            target_in_spam_count += len(re.findall(findword,all_text))\n",
    "            words_in_spam_count += len(all_text.split(' '))\n",
    "            spam_count += 1\n",
    "        \n",
    "        #not spam\n",
    "        else:\n",
    "            #check to see if the word is in the total corpus\n",
    "            for word in all_text.split(' '):\n",
    "                if word not in set_of_words_not_spam:\n",
    "                    set_of_words_not_spam = set_of_words_not_spam + word + \" \"\n",
    "            \n",
    "            #increment number of times seen in not_spam\n",
    "            target_in_not_spam_count += len(re.findall(findword,all_text))\n",
    "            words_in_not_spam_count += len(all_text.split(' '))\n",
    "            \n",
    "        email_count += 1\n",
    "\n",
    "#append email stats\n",
    "output_set.append(str(spam_count)+\",\"+str(email_count))\n",
    "\n",
    "#append word stats\n",
    "output_set.append(str(target_in_spam_count)+\",\"+str(target_in_not_spam_count))\n",
    "\n",
    "#append target stats\n",
    "output_set.append(str(words_in_spam_count)+\",\"+str(words_in_not_spam_count))\n",
    "\n",
    "#append spam words\n",
    "output_set.append(set_of_words_spam)\n",
    "\n",
    "#append total corpus\n",
    "output_set.append(set_of_words_not_spam)\n",
    "\n",
    "\n",
    "#print list of info\n",
    "for element in output_set:\n",
    "    print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "total_set_of_spam = \"\"\n",
    "total_set_of_not_spam = \"\"\n",
    "words_in_spam = 0\n",
    "words_in_not_spam = 0\n",
    "target_in_spam = 0\n",
    "target_in_not_spam = 0\n",
    "spam_count = 0\n",
    "email_count = 0\n",
    "\n",
    "#get set_of_spam words from each file and combine\n",
    "for filename in sys.argv[1:]:\n",
    "    \n",
    "    lines = open(filename).read().splitlines()\n",
    "    \n",
    "    #extract the data in each chunk\n",
    "    spam_count += int(lines[-5].split(',')[0])\n",
    "    email_count += int(lines[-5].split(',')[1])\n",
    "    words_in_spam += int(lines[-4].split(',')[0])\n",
    "    words_in_not_spam += int(lines[-4].split(',')[1])\n",
    "    target_in_spam = int(lines[-3].split(',')[0])\n",
    "    target_in_not_spam = int(lines[-3].split(',')[1])\n",
    "    set_of_spam = lines[-2]\n",
    "    set_of_not_spam = lines[-1]\n",
    "    \n",
    "    for word in set_of_spam.split(' '):\n",
    "        if word not in total_set_of_spam:\n",
    "            total_set_of_spam = total_set_of_spam + word + \" \"\n",
    "            \n",
    "    for word in set_of_not_spam.split(' '):\n",
    "        if word not in total_set_of_not_spam:\n",
    "            total_set_of_not_spam = total_set_of_not_spam + word + \" \"\n",
    "\n",
    "#going to total count of words in each class instead of unique\n",
    "total_words_in_spam = len(total_set_of_spam.split(' '))\n",
    "total_words_int_not_spam = len(total_set_of_not_spam.split(' '))\n",
    "\n",
    "p_spam = float(spam_count) / float(email_count)\n",
    "p_not_spam = 1 - p_spam\n",
    "\n",
    "\n",
    "#compute probability of spam vs not spam\n",
    "for filename in sys.argv[1:]:\n",
    "    lines = open(filename).read().splitlines()\n",
    "    \n",
    "    #remove the email stats, spam word set, and total vocab set\n",
    "    lines = lines[0:-5]\n",
    "    \n",
    "    for line in lines:\n",
    "        ID = line.split(',')[0]\n",
    "        target_count = line.split(',')[1]\n",
    "        #p_word = float(target_count) / float(total_words_unique) \n",
    "        p_cond_spam = float(target_in_spam) / float(total_words_in_spam)\n",
    "        p_cond_not_spam = float(target_in_not_spam) / float(total_words_int_not_spam)\n",
    "        \n",
    "        #P(SPAM | word) = [P(word | SPAM) * P(SPAM)] / P(word)\n",
    "        #why is the equation above wrong?\n",
    "        #log(P) = log(P(word|SPAM)) + log(P(SPAM)) - log(P(word))\n",
    "        \n",
    "        #log_prob_spam = math.log(p_cond_spam) + math.log(p_spam) - math.log(p_word)\n",
    "        #log_prob_not_spam = math.log(p_cond_not_spam) + math.log(p_not_spam) - math.log(p_word)\n",
    "\n",
    "        #P(SPAM | word) = p_spam * P(word|SPAM) * word_count\n",
    "        \n",
    "        log_prob_spam = math.log(p_spam) + math.log(p_cond_spam) * float(target_count)\n",
    "        log_prob_not_spam = math.log(p_not_spam) + math.log(p_cond_not_spam) * float(target_count)\n",
    "        \n",
    "        if log_prob_spam > log_prob_not_spam:\n",
    "            print ID + \",1,\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "        else:\n",
    "            print ID + \",0,\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " **Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results**\n",
    "   \n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py \n",
    "\n",
    "   performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "No smoothing is needed in this HW.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "##The following mapper reducer pair does organization in the mapper.\n",
    "##The output is a list with entries being (ID, target_word_count), and the last entry being a list of words in spam\n",
    "##The reducer takes the list of all words in all spam, creates a total list\n",
    "##Using the total list size and target_word_count, we compute the probability of spam\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    " \n",
    "lines = open(filename).read().splitlines()\n",
    "\n",
    "#no need to check lines for correct number of fields.  All have 4\n",
    "\n",
    "set_of_words_spam = \"\"\n",
    "set_of_words_not_spam = \"\"\n",
    "output_set = []\n",
    "spam_count = 0\n",
    "email_count = 0\n",
    "target_in_spam_count = 0\n",
    "target_in_not_spam_count = 0\n",
    "words_in_spam_count = 0\n",
    "words_in_not_spam_count = 0\n",
    "\n",
    "#append ID and target word count to output\n",
    "#get set of words in spam\n",
    "#get [spam email, email] count\n",
    "for line in lines:\n",
    "    if len(line.split('\\t')) == 4:\n",
    "        \n",
    "        target_word_count = 0\n",
    "        tmp = line.split('\\t')\n",
    "        all_text = tmp[2] + \" \" + tmp[3]\n",
    "        \n",
    "        #for each word, count number of times seen in line, and increment other counts\n",
    "        for target in findwords:\n",
    "            \n",
    "            #add times a target is seen\n",
    "            target_word_count += len(re.findall(target,all_text))\n",
    "\n",
    "            #if spam\n",
    "            if tmp[1] == \"1\":\n",
    "                #check to see if we've seen the word before in spam\n",
    "                for word in all_text.split(' '):\n",
    "                    if word not in set_of_words_spam:\n",
    "                        set_of_words_spam = set_of_words_spam + word + \" \"\n",
    "\n",
    "                #increment number of times seen in spam and total spam words\n",
    "                target_in_spam_count += len(re.findall(target,all_text))\n",
    "                words_in_spam_count += len(all_text.split(' '))\n",
    "                \n",
    "\n",
    "            #not spam\n",
    "            else:\n",
    "                #check to see if the word is in the total corpus\n",
    "                for word in all_text.split(' '):\n",
    "                    if word not in set_of_words_not_spam:\n",
    "                        set_of_words_not_spam = set_of_words_not_spam + word + \" \"\n",
    "\n",
    "                #increment number of times seen in not_spam\n",
    "                target_in_not_spam_count += len(re.findall(target,all_text))\n",
    "                words_in_not_spam_count += len(all_text.split(' '))\n",
    "\n",
    "        if tmp[1] == \"1\":\n",
    "            spam_count += 1\n",
    "\n",
    "        email_count += 1\n",
    "        \n",
    "        #append (ID, target count)\n",
    "        output_set.append(str(tmp[0] + \",\" + str(target_word_count)))\n",
    "\n",
    "#append email stats\n",
    "output_set.append(str(spam_count)+\",\"+str(email_count))\n",
    "\n",
    "#append word stats\n",
    "output_set.append(str(target_in_spam_count)+\",\"+str(target_in_not_spam_count))\n",
    "\n",
    "#append target stats\n",
    "output_set.append(str(words_in_spam_count)+\",\"+str(words_in_not_spam_count))\n",
    "\n",
    "#append spam words\n",
    "output_set.append(set_of_words_spam)\n",
    "\n",
    "#append total corpus\n",
    "output_set.append(set_of_words_not_spam)\n",
    "\n",
    "\n",
    "#print list of info\n",
    "for element in output_set:\n",
    "    print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "total_set_of_spam = \"\"\n",
    "total_set_of_not_spam = \"\"\n",
    "words_in_spam = 0\n",
    "words_in_not_spam = 0\n",
    "target_in_spam = 0\n",
    "target_in_not_spam = 0\n",
    "spam_count = 0\n",
    "email_count = 0\n",
    "\n",
    "#get set_of_spam words from each file and combine\n",
    "for filename in sys.argv[1:]:\n",
    "    \n",
    "    lines = open(filename).read().splitlines()\n",
    "    \n",
    "    print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "total_set_of_spam = \"\"\n",
    "total_set_of_not_spam = \"\"\n",
    "words_in_spam = 0\n",
    "words_in_not_spam = 0\n",
    "target_in_spam = 0\n",
    "target_in_not_spam = 0\n",
    "spam_count = 0\n",
    "email_count = 0\n",
    "\n",
    "#get set_of_spam words from each file and combine\n",
    "for filename in sys.argv[1:]:\n",
    "    \n",
    "    lines = open(filename).read().splitlines()\n",
    "    \n",
    "    #extract the data in each chunk\n",
    "    spam_count += int(lines[-5].split(',')[0])\n",
    "    email_count += int(lines[-5].split(',')[1])\n",
    "    words_in_spam += int(lines[-4].split(',')[0])\n",
    "    words_in_not_spam += int(lines[-4].split(',')[1])\n",
    "    target_in_spam = int(lines[-3].split(',')[0])\n",
    "    target_in_not_spam = int(lines[-3].split(',')[1])\n",
    "    set_of_spam = lines[-2]\n",
    "    set_of_not_spam = lines[-1]\n",
    "    \n",
    "    for word in set_of_spam.split(' '):\n",
    "        if word not in total_set_of_spam:\n",
    "            total_set_of_spam = total_set_of_spam + word + \" \"\n",
    "            \n",
    "    for word in set_of_not_spam.split(' '):\n",
    "        if word not in total_set_of_not_spam:\n",
    "            total_set_of_not_spam = total_set_of_not_spam + word + \" \"\n",
    "\n",
    "#going to total count of words in each class instead of unique\n",
    "total_words_in_spam = len(total_set_of_spam.split(' '))\n",
    "total_words_int_not_spam = len(total_set_of_not_spam.split(' '))\n",
    "\n",
    "p_spam = float(spam_count) / float(email_count)\n",
    "p_not_spam = 1 - p_spam\n",
    "\n",
    "\n",
    "#compute probability of spam vs not spam\n",
    "for filename in sys.argv[1:]:\n",
    "    lines = open(filename).read().splitlines()\n",
    "    \n",
    "    #remove the email stats, spam word set, and total vocab set\n",
    "    lines = lines[0:-5]\n",
    "    \n",
    "    for line in lines:\n",
    "        ID = line.split(',')[0]\n",
    "        target_count = line.split(',')[1]\n",
    "        #p_word = float(target_count) / float(total_words_unique) \n",
    "        p_cond_spam = float(target_in_spam) / float(total_words_in_spam)\n",
    "        p_cond_not_spam = float(target_in_not_spam) / float(total_words_int_not_spam)\n",
    "        \n",
    "        #P(SPAM | word) = [P(word | SPAM) * P(SPAM)] / P(word)\n",
    "        #why is the equation above wrong?\n",
    "        #log(P) = log(P(word|SPAM)) + log(P(SPAM)) - log(P(word))\n",
    "        \n",
    "        #log_prob_spam = math.log(p_cond_spam) + math.log(p_spam) - math.log(p_word)\n",
    "        #log_prob_not_spam = math.log(p_cond_not_spam) + math.log(p_not_spam) - math.log(p_word)\n",
    "\n",
    "        #P(SPAM | word) = p_spam * P(word|SPAM) * word_count\n",
    "        \n",
    "        log_prob_spam = math.log(p_spam) + math.log(p_cond_spam) * float(target_count)\n",
    "        log_prob_not_spam = math.log(p_not_spam) + math.log(p_cond_not_spam) * float(target_count)\n",
    "        \n",
    "        if log_prob_spam > log_prob_not_spam:\n",
    "            print ID + \",1,\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "        else:\n",
    "            print ID + \",0,\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
