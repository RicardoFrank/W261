{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.5.1 (default, Dec  7 2015 11:24:55)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ['SPARK_HOME'] = '/opt/spark-1.5.0-bin-hadoop2.6/'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "exec(open(os.path.join(spark_home,'python/pyspark/shell.py')).read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing callsign_tbl_sorted.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile callsign_tbl_sorted.txt\n",
    "3AZ, Monaco (Principality of)\n",
    "3BZ, Mauritius (Republic of)\n",
    "3CZ, Equatorial Guinea (Republic of)\n",
    "3DM, Swaziland (Kingdom of)\n",
    "3DZ, Fiji (Republic of)\n",
    "3FZ, Panama (Republic of)\n",
    "3GZ, Chile\n",
    "3UZ, China (People's Republic of)\n",
    "3VZ, Tunisia\n",
    "3WZ, Viet Nam (Socialist Republic of)\n",
    "3XZ, Guinea (Republic of)\n",
    "3YZ, Norway\n",
    "3ZZ, Poland (Republic of)\n",
    "4CZ, Mexico\n",
    "4IZ, Philippines (Republic of the)\n",
    "4KZ, Azerbaijani Republic\n",
    "4LZ, Georgia (Republic of)\n",
    "4MZ, Venezuela (Republic of)\n",
    "4OZ, Montenegro (Republic of)                    \t(WRC-07)\n",
    "4SZ, Sri Lanka (Democratic Socialist Republic of)\n",
    "4TZ, Peru\n",
    "4UZ, United Nations\n",
    "4VZ, Haiti (Republic of)\n",
    "4WZ, Democratic Republic of Timor-Leste   (WRC-03)\n",
    "4XZ, Israel (State of)\n",
    "4YZ, International Civil Aviation Organization\n",
    "4ZZ, Israel (State of)\n",
    "5AZ, Libya (Socialist People's Libyan Arab Jamahiriya)\n",
    "5BZ, Cyprus (Republic of)\n",
    "5GZ, Morocco (Kingdom of)\n",
    "5IZ, Tanzania (United Republic of)\n",
    "5KZ, Colombia (Republic of)\n",
    "5MZ, Liberia (Republic of)\n",
    "5OZ, Nigeria (Federal Republic of)\n",
    "5QZ, Denmark\n",
    "5SZ, Madagascar (Republic of)\n",
    "5TZ, Mauritania (Islamic Republic of)\n",
    "5UZ, Niger (Republic of the)\n",
    "5VZ, Togolese Republic\n",
    "5WZ, Samoa (Independent State of)\n",
    "5XZ, Uganda (Republic of)\n",
    "5ZZ, Kenya (Republic of)\n",
    "6BZ, Egypt (Arab Republic of)\n",
    "6CZ, Syrian Arab Republic\n",
    "6JZ, Mexico\n",
    "6NZ, Korea (Republic of)\n",
    "6OZ, Somali Democratic Republic\n",
    "6SZ, Pakistan (Islamic Republic of)\n",
    "6UZ, Sudan (Republic of the)\n",
    "6WZ, Senegal (Republic of)\n",
    "6XZ, Madagascar (Republic of)\n",
    "6YZ, Jamaica\n",
    "6ZZ, Liberia (Republic of)\n",
    "7IZ, Indonesia (Republic of)\n",
    "7NZ, Japan\n",
    "7OZ, Yemen (Republic of)\n",
    "7PZ, Lesotho (Kingdom of)\n",
    "7QZ, Malawi\n",
    "7RZ, Algeria (People's Democratic Republic of)\n",
    "7SZ, Sweden\n",
    "7YZ, Algeria (People's Democratic Republic of)\n",
    "7ZZ, Saudi Arabia (Kingdom of)\n",
    "8IZ, Indonesia (Republic of)\n",
    "8NZ, Japan\n",
    "8OZ, Botswana (Republic of)\n",
    "8PZ, Barbados\n",
    "8QZ, Maldives (Republic of)\n",
    "8RZ, Guyana\n",
    "8SZ, Sweden\n",
    "8YZ, India (Republic of)\n",
    "8ZZ, Saudi Arabia (Kingdom of)\n",
    "9AZ, Croatia (Republic of)\n",
    "9DZ, Iran (Islamic Republic of)\n",
    "9FZ, Ethiopia (Federal Democratic Republic of)\n",
    "9GZ, Ghana\n",
    "9HZ, Malta\n",
    "9JZ, Zambia (Republic of)\n",
    "9KZ, Kuwait (State of)\n",
    "9LZ, Sierra Leone\n",
    "9MZ, Malaysia\n",
    "9NZ, Nepal\n",
    "9TZ, Democratic Republic of the Congo\n",
    "9UZ, Burundi (Republic of)\n",
    "9VZ, Singapore (Republic of)\n",
    "9WZ, Malaysia\n",
    "9XZ, Rwandese Republic\n",
    "9ZZ, Trinidad and Tobago\n",
    "A2Z, Botswana (Republic of)\n",
    "A3Z, Tonga (Kingdom of)\n",
    "A4Z, Oman (Sultanate of)\n",
    "A5Z, Bhutan (Kingdom of)\n",
    "A6Z, United Arab Emirates\n",
    "A7Z, Qatar (State of)\n",
    "A8Z, Liberia (Republic of)\n",
    "A9Z, Bahrain (State of)\n",
    "ALZ, United States of America\n",
    "AOZ, Spain\n",
    "ASZ, Pakistan (Islamic Republic of)\n",
    "AWZ, India (Republic of)\n",
    "AXZ, Australia\n",
    "AZZ, Argentine Republic\n",
    "BZZ, China (People's Republic of)\n",
    "C2Z, Nauru (Republic of)\n",
    "C3Z, Andorra (Principality of)\n",
    "C4Z, Cyprus (Republic of)\n",
    "C5Z, Gambia (Republic of the)\n",
    "C6Z, Bahamas (Commonwealth of the)\n",
    "C7Z, World Meteorological Organization\n",
    "C9Z, Mozambique (Republic of)\n",
    "CEZ, Chile\n",
    "CKZ, Canada\n",
    "CMZ, Cuba\n",
    "CNZ, Morocco (Kingdom of)\n",
    "COZ, Cuba\n",
    "CPZ, Bolivia (Republic of)\n",
    "CUZ, Portugal\n",
    "CXZ, Uruguay (Eastern Republic of)\n",
    "CZZ, Canada\n",
    "D3Z, Angola (Republic of)\n",
    "D4Z, Cape Verde (Republic of)\n",
    "D5Z, Liberia (Republic of)\n",
    "D6Z, Comoros (Islamic Federal Republic of the)\n",
    "D9Z, Korea (Republic of)\n",
    "DRZ, Germany (Federal Republic of)\n",
    "DTZ, Korea (Republic of)\n",
    "DZZ, Philippines (Republic of the)\n",
    "E2Z, Thailand\n",
    "E3Z, Eritrea\n",
    "E4Z, Palestinian Authority\n",
    "E5Z, New Zealand - Cook Islands                      \t(WRC-07)\n",
    "E6Z, New Zealand - Niue\n",
    "E7Z, Bosnia and Herzegovina (Republic of)         \t(WRC-07)\n",
    "EHZ, Spain\n",
    "EJZ, Ireland\n",
    "EKZ, Armenia (Republic of)\n",
    "ELZ, Liberia (Republic of)\n",
    "EOZ, Ukraine\n",
    "EQZ, Iran (Islamic Republic of)\n",
    "ERZ, Moldova (Republic of)\n",
    "ESZ, Estonia (Republic of)\n",
    "ETZ, Ethiopia (Federal Democratic Republic of)\n",
    "EWZ, Belarus (Republic of)\n",
    "EXZ, Kyrgyz Republic\n",
    "EYZ, Tajikistan (Republic of)\n",
    "EZZ, Turkmenistan\n",
    "FZZ, France\n",
    "GZZ, United Kingdom of Great Britain and Northern Ireland\n",
    "H2Z, Cyprus (Republic of)\n",
    "H3Z, Panama (Republic of)\n",
    "H4Z, Solomon Islands\n",
    "H7Z, Nicaragua\n",
    "H9Z, Panama (Republic of)\n",
    "HAZ, Hungary (Republic of)\n",
    "HBZ, Switzerland (Confederation of)\n",
    "HDZ, Ecuador\n",
    "HEZ, Switzerland (Confederation of)\n",
    "HFZ, Poland (Republic of)\n",
    "HGZ, Hungary (Republic of)\n",
    "HHZ, Haiti (Republic of)\n",
    "HIZ, Dominican Republic\n",
    "HKZ, Colombia (Republic of)\n",
    "HLZ, Korea (Republic of)\n",
    "HMZ, Democratic People's Republic of Korea\n",
    "HNZ, Iraq (Republic of)\n",
    "HPZ, Panama (Republic of)\n",
    "HRZ, Honduras (Republic of)\n",
    "HSZ, Thailand\n",
    "HTZ, Nicaragua\n",
    "HUZ, El Salvador (Republic of)\n",
    "HVZ, Vatican City State\n",
    "HYZ, France\n",
    "HZZ, Saudi Arabia (Kingdom of)\n",
    "IZZ, Italy\n",
    "J2Z, Djibouti (Republic of)\n",
    "J3Z, Grenada\n",
    "J4Z, Greece\n",
    "J5Z, Guinea-Bissau (Republic of)\n",
    "J6Z, Saint Lucia\n",
    "J7Z, Dominica (Commonwealth of)\n",
    "J8Z, Saint Vincent and the Grenadines\n",
    "JSZ, Japan\n",
    "JVZ, Mongolia\n",
    "JXZ, Norway\n",
    "JYZ, Jordan (Hashemite Kingdom of)\n",
    "JZZ, Indonesia (Republic of)\n",
    "KZZ, United States of America\n",
    "L9Z, Argentine Republic\n",
    "LNZ, Norway\n",
    "LWZ, Argentine Republic\n",
    "LXZ, Luxembourg\n",
    "LYZ, Lithuania (Republic of)\n",
    "LZZ, Bulgaria (Republic of)\n",
    "MZZ, United Kingdom of Great Britain and Northern Ireland\n",
    "NZZ, United States of America\n",
    "OCZ, Peru\n",
    "ODZ, Lebanon\n",
    "OEZ, Austria\n",
    "OJZ, Finland\n",
    "OLZ, Czech Republic\n",
    "OMZ, Slovak Republic\n",
    "OTZ, Belgium\n",
    "OZZ, Denmark\n",
    "P2Z, Papua New Guinea\n",
    "P3Z, Cyprus (Republic of)\n",
    "P4Z, Netherlands (Kingdom of the) - Aruba\n",
    "P9Z, Democratic People's Republic of Korea\n",
    "PIZ, Netherlands (Kingdom of the)\n",
    "PJZ, Netherlands (Kingdom of the) - Netherlands Caribbean\n",
    "POZ, Indonesia (Republic of)\n",
    "PYZ, Brazil (Federative Republic of)\n",
    "PZZ, Suriname (Republic of)\n",
    "RZZ, Russian Federation\n",
    "S3Z, Bangladesh (People's Republic of)\n",
    "S5Z, Slovenia (Republic of)\n",
    "S6Z, Singapore (Republic of)\n",
    "S7Z, Seychelles (Republic of)\n",
    "S8Z, South Africa (Republic of)\n",
    "S9Z, Sao Tome and Principe (Democratic Republic of)\n",
    "SMZ, Sweden\n",
    "SRZ, Poland (Republic of)\n",
    "SSM, Egypt (Arab Republic of)\n",
    "STZ, Sudan (Republic of the)\n",
    "SUZ, Egypt (Arab Republic of)\n",
    "SZZ, Greece\n",
    "T2Z, Tuvalu\n",
    "T3Z, Kiribati (Republic of)\n",
    "T4Z, Cuba\n",
    "T5Z, Somali Democratic Republic\n",
    "T6Z, Afghanistan (Islamic State of)\n",
    "T7Z, San Marino (Republic of)\n",
    "T8Z, Palau (Republic of)\n",
    "TCZ, Turkey\n",
    "TDZ, Guatemala (Republic of)\n",
    "TEZ, Costa Rica\n",
    "TFZ, Iceland\n",
    "TGZ, Guatemala (Republic of)\n",
    "THZ, France\n",
    "TIZ, Costa Rica\n",
    "TJZ, Cameroon (Republic of)\n",
    "TKZ, France\n",
    "TLZ, Central African Republic\n",
    "TMZ, France\n",
    "TNZ, Congo (Republic of the)\n",
    "TQZ, France\n",
    "TRZ, Gabonese Republic\n",
    "TSZ, Tunisia\n",
    "TTZ, Chad (Republic of)\n",
    "TUZ, CÃ´te d'Ivoire (Republic of)\n",
    "TXZ, France\n",
    "TYZ, Benin (Republic of)\n",
    "TZZ, Mali (Republic of)\n",
    "UIZ, Russian Federation\n",
    "UMZ, Uzbekistan (Republic of)\n",
    "UQZ, Kazakhstan (Republic of)\n",
    "UZZ, Ukraine\n",
    "V2Z, Antigua and Barbuda\n",
    "V3Z, Belize\n",
    "V4Z, Saint Kitts and Nevis\n",
    "V5Z, Namibia (Republic of)\n",
    "V6Z, Micronesia (Federated States of)\n",
    "V7Z, Marshall Islands (Republic of the)\n",
    "V8Z, Brunei Darussalam\n",
    "VGZ, Canada\n",
    "VNZ, Australia\n",
    "VOZ, Canada\n",
    "VQZ, United Kingdom of Great Britain and Northern Ireland\n",
    "VRZ, China (People's Republic of) - Hong Kong\n",
    "VSZ, United Kingdom of Great Britain and Northern Ireland\n",
    "VWZ, India (Republic of)\n",
    "VYZ, Canada\n",
    "VZZ, Australia\n",
    "WZZ, United States of America\n",
    "XIZ, Mexico\n",
    "XOZ, Canada\n",
    "XPZ, Denmark\n",
    "XRZ, Chile\n",
    "XSZ, China (People's Republic of)\n",
    "XTZ, Burkina Faso\n",
    "XUZ, Cambodia (Kingdom of)\n",
    "XVZ, Viet Nam (Socialist Republic of)\n",
    "XWZ, Lao People's Democratic Republic\n",
    "XXZ, China (People's Republic of) - Macao         \t(WRC-07)\n",
    "XZZ, Myanmar (Union of)\n",
    "Y9Z, Germany (Federal Republic of)\n",
    "YAZ, Afghanistan (Islamic State of)\n",
    "YHZ, Indonesia (Republic of)\n",
    "YIZ, Iraq (Republic of)\n",
    "YJZ, Vanuatu (Republic of)\n",
    "YKZ, Syrian Arab Republic\n",
    "YLZ, Latvia (Republic of)\n",
    "YMZ, Turkey\n",
    "YNZ, Nicaragua\n",
    "YRZ, Romania\n",
    "YSZ, El Salvador (Republic of)\n",
    "YUZ, Serbia (Republic of)                                   \t(WRC-07)\n",
    "YYZ, Venezuela (Republic of)\n",
    "Z2Z, Zimbabwe (Republic of)\n",
    "Z3Z, The Former Yugoslav Republic of Macedonia\n",
    "Z8Z, South Sudan (Republic of)\n",
    "ZAZ, Albania (Republic of)\n",
    "ZJZ, United Kingdom of Great Britain and Northern Ireland\n",
    "ZMZ, New Zealand\n",
    "ZOZ, United Kingdom of Great Britain and Northern Ireland\n",
    "ZPZ, Paraguay (Republic of)\n",
    "ZQZ, United Kingdom of Great Britain and Northern Ireland\n",
    "ZUZ, South Africa (Republic of)\n",
    "ZZZ, Brazil (Federative Republic of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#..... Other code...\n",
    "#Country lookup code\n",
    "\n",
    "# Helper functions for looking up the call signs\n",
    "\n",
    "def lookupCountry(sign, prefixes):\n",
    "    pos = bisect.bisect_left(prefixes, sign)\n",
    "    return prefixes[pos].split(\",\")[1]\n",
    "\n",
    "\n",
    "def loadCallSignTable():\n",
    "    f = open(\"callsign_tbl_sorted.txt\", \"r\")\n",
    "    return f.readlines()\n",
    "\n",
    "# Lookup the locations of the call signs on the\n",
    "# RDD contactCounts. We load a list of call sign\n",
    "# prefixes to country code to support this lookup.\n",
    "signPrefixes = loadCallSignTable()\n",
    "\n",
    "\n",
    "def processSignCount(sign_count, signPrefixes):\n",
    "    country = lookupCountry(sign_count[0], signPrefixes)\n",
    "    count = sign_count[1]\n",
    "    return (country, count)\n",
    "\n",
    "contactCounts = sc.parallelize([[\"ZMZ\", 1], [\"ZMZ\", 3]])\n",
    "\n",
    "countryContactCounts = (contactCounts\n",
    "                        .map(lambda signCount: processSignCount(signCount, signPrefixes))\n",
    "                        .reduceByKey((lambda x, y: x + y)))\n",
    "\n",
    "#countryContactCounts.saveAsTextFile(\"tmp/countries.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing beerSales.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile beerSales.txt\n",
    "Week\tPRICE12PK\tPRICE18PK\tPRICE30PK\tCASES12PK\tCASES18PK\tCASES30PK\n",
    "1\t19.98\t14.10\t15.19\t223.5\t439\t55.00\n",
    "2\t19.98\t18.65\t15.19\t215.0\t98\t66.75\n",
    "3\t19.98\t18.65\t13.87\t227.5\t70\t242.00\n",
    "4\t19.98\t18.65\t12.83\t244.5\t52\t488.50\n",
    "5\t19.98\t18.65\t13.16\t313.5\t64\t308.75\n",
    "6\t19.98\t18.65\t15.19\t279.0\t72\t111.75\n",
    "7\t19.98\t18.65\t13.92\t238.0\t47\t252.50\n",
    "8\t20.10\t18.73\t14.42\t315.5\t85\t221.25\n",
    "9\t20.12\t18.75\t13.83\t217.0\t59\t245.25\n",
    "10\t20.13\t18.75\t14.50\t209.5\t63\t148.50\n",
    "11\t20.14\t18.75\t13.87\t227.0\t57\t229.75\n",
    "12\t20.12\t18.75\t13.64\t216.5\t54\t312.00\n",
    "13\t20.12\t13.87\t14.31\t169.0\t404\t96.75\n",
    "14\t20.13\t14.27\t13.85\t178.0\t380\t123.25\n",
    "15\t20.14\t18.76\t14.20\t301.5\t65\t200.50\n",
    "16\t20.14\t18.77\t13.64\t266.5\t40\t359.75\n",
    "17\t20.13\t13.87\t14.33\t182.5\t456\t113.50\n",
    "18\t20.13\t14.14\t13.14\t159.0\t176\t136.50\n",
    "19\t20.13\t18.76\t13.81\t285.5\t61\t225.50\n",
    "20\t20.13\t18.72\t15.19\t360.0\t91\t122.25\n",
    "21\t20.13\t18.76\t13.13\t263.0\t59\t443.75\n",
    "22\t19.18\t18.76\t13.63\t443.5\t83\t322.75\n",
    "23\t14.78\t18.74\t15.19\t1101.5\t41\t53.00\n",
    "24\t16.04\t18.75\t13.89\t814.0\t47\t140.75\n",
    "25\t20.12\t18.75\t14.28\t365.0\t84\t210.75\n",
    "26\t19.75\t18.75\t15.19\t510.0\t85\t110.50\n",
    "27\t19.65\t18.75\t13.12\t580.5\t116\t568.25\n",
    "28\t19.69\t13.79\t13.78\t251.0\t544\t115.50\n",
    "29\t20.12\t13.49\t15.19\t237.0\t890\t58.75\n",
    "30\t20.12\t14.89\t15.19\t302.5\t371\t77.25\n",
    "31\t20.13\t13.94\t15.19\t229.5\t557\t66.25\n",
    "32\t20.14\t13.67\t15.19\t188.5\t775\t50.00\n",
    "33\t15.14\t14.43\t15.19\t795.5\t236\t46.50\n",
    "34\t14.33\t18.75\t15.19\t1556.5\t43\t65.75\n",
    "35\t16.24\t18.22\t13.14\t807.5\t63\t252.75\n",
    "36\t19.93\t14.06\t13.45\t243.0\t469\t179.00\n",
    "37\t21.06\t14.43\t13.00\t201.5\t335\t226.25\n",
    "38\t21.19\t19.48\t13.60\t294.0\t75\t288.50\n",
    "39\t21.23\t15.15\t14.46\t220.5\t461\t114.25\n",
    "40\t20.12\t13.79\t14.94\t255.5\t817\t70.00\n",
    "41\t14.73\t14.31\t15.19\t920.5\t200\t47.75\n",
    "42\t14.57\t19.50\t15.19\t730.0\t32\t98.75\n",
    "43\t15.94\t13.85\t15.19\t262.5\t460\t77.00\n",
    "44\t20.70\t14.23\t13.43\t209.5\t751\t160.50\n",
    "45\t19.57\t19.31\t14.37\t283.0\t70\t143.50\n",
    "46\t19.60\t19.29\t15.19\t262.5\t80\t133.00\n",
    "47\t19.94\t13.76\t15.19\t310.0\t523\t68.75\n",
    "48\t21.28\t13.45\t15.19\t278.5\t741\t81.75\n",
    "49\t14.56\t15.13\t15.19\t741.5\t130\t56.25\n",
    "50\t14.39\t19.43\t15.19\t1316.0\t69\t68.75\n",
    "51\t16.81\t13.26\t15.19\t449.0\t493\t49.25\n",
    "52\t19.86\t13.92\t15.19\t505.0\t814\t76.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=sc.textFile(\"beerSales.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Week\\tPRICE12PK\\tPRICE18PK\\tPRICE30PK\\tCASES12PK\\tCASES18PK\\tCASES30PK']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[439,\n",
       " 98,\n",
       " 70,\n",
       " 52,\n",
       " 64,\n",
       " 72,\n",
       " 47,\n",
       " 85,\n",
       " 59,\n",
       " 63,\n",
       " 57,\n",
       " 54,\n",
       " 404,\n",
       " 380,\n",
       " 65,\n",
       " 40,\n",
       " 456,\n",
       " 176,\n",
       " 61,\n",
       " 91,\n",
       " 59,\n",
       " 83,\n",
       " 41,\n",
       " 47,\n",
       " 84,\n",
       " 85,\n",
       " 116,\n",
       " 544,\n",
       " 890,\n",
       " 371,\n",
       " 557,\n",
       " 775,\n",
       " 236,\n",
       " 43,\n",
       " 63,\n",
       " 469,\n",
       " 335,\n",
       " 75,\n",
       " 461,\n",
       " 817,\n",
       " 200,\n",
       " 32,\n",
       " 460,\n",
       " 751,\n",
       " 70,\n",
       " 80,\n",
       " 523,\n",
       " 741,\n",
       " 130,\n",
       " 69,\n",
       " 493,\n",
       " 814]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "def combineCtrs(c1, c2):\n",
    "    return (c1[0] + c2[0], c1[1] + c2[1])\n",
    "def basicAvg(nums):\n",
    "    \"\"\"Compute the average\"\"\"\n",
    "    nums.map(lambda num: (num, 1)).reduce(combineCtrs)\n",
    "\n",
    "def partitionCtr(nums):\n",
    "    \"\"\"Compute sumCounter for partition\"\"\"\n",
    "    sumCount = [0, 0]\n",
    "    for num in nums:\n",
    "        sumCount[0] += num\n",
    "        sumCount[1] += 1\n",
    "    return [sumCount]\n",
    "\n",
    "def fastAvg(nums):\n",
    "    \"\"\"Compute the avg\"\"\"\n",
    "    sumCount = nums.mapPartitions(partitionCtr).reduce(combineCtrs)\n",
    "    return sumCount[0] / float(sumCount[1])\n",
    "\n",
    "nums = fastAvg(sc.parallelize([1,2,3,4]))\n",
    "print(nums)\n",
    "\n",
    "df_Case18 = df.map(lambda row: row.split()[5]).filter(lambda entry: entry != 'CASES18PK').cache()\n",
    "\n",
    "df_Case18_int = df_Case18.map(lambda x: int(x)).cache()\n",
    "\n",
    "df_Case18_int.collect()\n",
    "#f = sqlc.createDataFrame(df_Case18_int, ['dollars'])\n",
    "\n",
    "#averages_rdd = df.groupby('dollars').agg(func.avg('dollars').alias('avg_dollars'))\n",
    "\n",
    "#df_18_avg = df_Case18.map(fastAvg).cache()\n",
    "\n",
    "#df_18_avg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0b98093e144b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Week'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PRICE12PK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PRICE18PK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PRICE30PK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CASES12PK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CASES18PK'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CASES30PK'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sqlContext.createDataFrame(df, ['Week', 'PRICE12PK', 'PRICE18PK', 'PRICE30PK', 'CASES12PK', 'CASES18PK','CASES30PK']).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ab0a7d603bd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-1.5.0-bin-hadoop2.6/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "sqlContext.createDataFrame(df).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
