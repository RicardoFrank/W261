{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #2=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #2  (version 2016-01-26)\n",
    "\n",
    "---------------\n",
    "=== INSTRUCTIONS for SUBMISSIONS ===\n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "\n",
    "=== Week 2 ASSIGNMENTS ===\n",
    "\n",
    "Ricardo Frank Barrera\n",
    "\n",
    "RicardoFrankBarrera@gmail.com\n",
    "\n",
    "W261 - Group 2\n",
    "\n",
    "Week 2\n",
    "\n",
    "Submitted 01/26/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a race condition in the context of parallel computation**\n",
    "A race condition is a non-deterministic process, largely due to multiple threads sharing memory / variables and not having synchronization / blocking / barriers to ensure a deterministic execution process.\n",
    "\n",
    "Please see the example below:\n",
    "\n",
    "![Example image for selecting optimal model](RaceCondition.png)\n",
    "\n",
    "**What is MapReduce**\n",
    "MapReduce is a computation paradigm that uses (key,value) processing to organize computation in a distributed fashion.  A MapReduce job is broken down into a Mapper and Reducer, and the Reducer is supposed to receive all keys of a certain type to Reduce (e.g. aggregate).\n",
    "\n",
    "Please see the example below:\n",
    "\n",
    "![Example image for selecting optimal model](MapReduce.png)\n",
    "\n",
    "**How does it differ from Hadoop**\n",
    "Hadoop is a system that uses HDFS (Hadoop File System) and other computation frameworks (e.g. Hive) to perform batch processing in a MapReduce fashion. \n",
    "\n",
    "So, the Hadoop ecosystem provides many flavors of MapReduce\n",
    "\n",
    "**Which programming paradigm is Hadoop based on? Explain and give an example in code and show the code running**\n",
    "Hadoop is based upon MapReduce, as stated above.  As for examples, please refer to all of the code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given as input: Records of the form \\<integer, “NA”\\>, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form \\<integer, “NA”\\> in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.**\n",
    "\n",
    "**Write code to generate N  random records of the form \\<integer, “NA”\\>. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting random_gen.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile random_gen.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "num_vals = int(sys.argv[1])\n",
    "\n",
    "while num_vals > 0:\n",
    "        print \"<\"+str(random.random()*100)+\",\\\"\\\">\"\n",
    "        num_vals -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing assistance_wc_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile assistance_wc_mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "#split line into words and print out if target is found for reduce\n",
    "        for element in re.findall('assistance', line):\n",
    "                print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting assistance_wc_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile assistance_wc_reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "word_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "        if line != (\"\" or \"\\n\"):\n",
    "                word_count += 1\n",
    "\n",
    "print word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x random_gen.py; chmod +x assistance_wc_mapper.py; chmod +x assistance_wc_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sort_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sort_mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "REGEX_NUM = re.compile('[0-9]+')\n",
    "total_num_list = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "        total_num_list.append(float(re.search(REGEX_NUM,line).group()))\n",
    "\n",
    "total_num_list.sort()\n",
    "\n",
    "for element in total_num_list:\n",
    "        print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top_10_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_10_reducer.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "top_10_counter = 10\n",
    "total_lines = 0\n",
    "line_counter = 0\n",
    "tmp = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "        line_counter += 1\n",
    "        tmp.append(line)\n",
    "\n",
    "total_lines = line_counter\n",
    "line_counter = 0\n",
    "\n",
    "for line in tmp:\n",
    "\n",
    "        #print top 10 or bottom ten\n",
    "        if top_10_counter > 0:\n",
    "                print \"<\" + str(line.strip('\\n')) + \",\\\"\\\">\"\n",
    "                top_10_counter -= 1\n",
    "                line_counter += 1\n",
    "\n",
    "        elif (total_lines - line_counter) > 10:\n",
    "                line_counter += 1\n",
    "\n",
    "        else:\n",
    "                print \"<\" + line.strip('\\n') + \",\\\"\\\">\"\n",
    "                line_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HW21.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW21.sh\n",
    "\n",
    "python random_gen.py 10000 > 10000_rand_list\n",
    "hdfs dfs -rm /tmp/10000_rand_list\n",
    "hdfs dfs -put 10000_rand_list /tmp/10000_rand_list\n",
    "hdfs dfs -rmdir -r /tmp/output_HW21\n",
    "hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.5.jar -file ~/test/sort_mapper.py ~/test/top_10_reducer.py -mapper \"python sort_mapper.py\" -reducer \"python top_10_reducer.py\" -input /tmp/10000_rand_list -output /tmp/output_HW21\n",
    "hdfs dfs -cat /tmp/output_HW21/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x sort_mapper.py; chmod +x top_10_reducer.py; chmod +x HW21.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<0.0,\"\">\r\n",
      "<0.0,\"\">\r\n",
      "<0.0,\"\">\r\n",
      "<0.0,\"\">\r\n",
      "<0.0,\"\">\r\n",
      "<0.0,\"\">\r\n",
      "<0.0,\"\">\r\n",
      "<0.0,\"\">\r\n",
      "<0.0,\"\">\r\n",
      "<0.0,\"\">\r\n",
      "<99.0,\"\">\r\n",
      "<99.0,\"\">\r\n",
      "<99.0,\"\">\r\n",
      "<99.0,\"\">\r\n",
      "<99.0,\"\">\r\n",
      "<99.0,\"\">\r\n",
      "<99.0,\"\">\r\n",
      "<99.0,\"\">\r\n",
      "<99.0,\"\">\r\n",
      "<99.0,\"\">\r\n"
     ]
    }
   ],
   "source": [
    "!python random_gen.py 10000 > 10000_rand_list\n",
    "!cat 10000_rand_list| python sort_mapper.py | python top_10_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.**\n",
    "\n",
    " \n",
    "**CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d\\$'\\t' -f4| grep assistance|wc -l    \n",
    "       8    \n",
    "       **#NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!\n",
    "\n",
    "**HW2.2.1  Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wc_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wc_mapper.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#in the mapper, we take each line and split\n",
    "#and we also count words at the end to reduce\n",
    "#transfer to the reducer\n",
    "\n",
    "word_list = []\n",
    "word_seen = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    #grab only content for word count\n",
    "    tmp = line.split('\\t')\n",
    "\n",
    "    if len(tmp) != 4:\n",
    "        content = tmp[-1]\n",
    "    else:\n",
    "        content = tmp[-2] + tmp[-1]\n",
    "\n",
    "        #split line into words and add word if new and add to list\n",
    "        for element in re.split(\"[, \\.\\n]+\", content):\n",
    "\n",
    "            if element not in word_seen:\n",
    "                word_seen.append(element)\n",
    "            word_list.append(element)\n",
    "\n",
    "#prepare for counting\n",
    "\n",
    "word_list.sort()\n",
    "word_seen.sort()\n",
    "cur_pos = 0\n",
    "cur_count = 0\n",
    "word_count = {}\n",
    "end_of_list = False\n",
    "\n",
    "#go through word-by-word counting until next word is seen\n",
    "for word in word_seen:\n",
    "    while (word_list[cur_pos] == word) and (end_of_list is False):\n",
    "        cur_count += 1\n",
    "\n",
    "        if cur_pos == (len(word_list) - 1):\n",
    "            end_of_list = True\n",
    "\n",
    "        else:\n",
    "            cur_pos += 1\n",
    "\n",
    "    word_count[word] = cur_count\n",
    "    cur_count = 0\n",
    "\n",
    "#print word_count\n",
    "for key, value in word_count.items():\n",
    "    print str(key + '\\t' + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wc_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wc_reducer.py\n",
    "\n",
    "import sys\n",
    "import operator\n",
    "import re\n",
    "\n",
    "#build reducer able to handle input from multiple mappers\n",
    "#input comes in as \"word,count\"\n",
    "#build a vocab, sort, and then reduce\n",
    "\n",
    "total_wc_list = []\n",
    "\n",
    "#for some reason my mapper outputs a new line between each entry\n",
    "#so just fixing that nonsense the stupid way\n",
    "\n",
    "for line in sys.stdin:\n",
    "    if line != (\"\" or \"\\n\"):\n",
    "        total_wc_list.append(line)\n",
    "\n",
    "total_wc_list.sort()\n",
    "#print total_wc_list\n",
    "\n",
    "cur_word = \"\"\n",
    "cur_sum = 0\n",
    "first_word = True\n",
    "output = {}\n",
    "\n",
    "#re-aggregate the counts\n",
    "for entry in total_wc_list:\n",
    "\n",
    "    if len(entry.split('\\t')) != 2:\n",
    "            print entry\n",
    "    word = entry.split('\\t')[0]\n",
    "        #count = re.sub(\"[^0-9]\",\"\",entry.split('\\t')[1])\n",
    "    count = int(re.sub(\"\\n\",\"\",entry.split('\\t')[-1]))\n",
    "        #print word+\",\"+str(count)\n",
    "\n",
    "    if word in output:\n",
    "        output[word] += count\n",
    "    else:\n",
    "        output[word] = count\n",
    "\n",
    "#pick the top 10.  Could be done more efficiently by tracking\n",
    "#while aggregating above, but the section below is easy and cheap\n",
    "\n",
    "sorted_out = sorted(output.items(), key = lambda x: int(x[1]), reverse=True)\n",
    "\n",
    "cur_count = 0\n",
    "\n",
    "for key, value in sorted_out:\n",
    "    print key +'\\t'+ str(value)\n",
    "    cur_count += 1\n",
    "\n",
    "    if cur_count == 10:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW22.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW22.sh\n",
    "\n",
    "hdfs dfs -rm /tmp/enronemail_1h.txt\n",
    "hdfs dfs -put enronemail_1h.txt /tmp/enronemail_1h.txt\n",
    "hdfs dfs -rm -r /tmp/output_HW22\n",
    "hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.5.jar -file ~/test/wc_mapper.py ~/test/wc_reducer.py -mapper \"python wc_mapper.py\" -reducer \"python wc_reducer.py\" -input /tmp/enronemail_1h.txt -output /tmp/output_HW22\n",
    "hdfs dfs -cat /tmp/output_HW22/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x wc_mapper.py; chmod +x wc_reducer.py; chmod +x HW22.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1240\r\n",
      "to\t914\r\n",
      "and\t659\r\n",
      "of\t556\r\n",
      "a\t527\r\n",
      "in\t415\r\n",
      "you\t407\r\n",
      "your\t389\r\n",
      "for\t369\r\n",
      "@\t361\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | python wc_mapper.py | python wc_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nb_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nb_mapper.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "\n",
    "input = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    input.append(line)\n",
    "        #matches = re.findall('\\bassistance\\b',line)\n",
    "        #if len(matches) > 0:\n",
    "        #       print matches\n",
    "\n",
    "output = {}\n",
    "output[\"1\"] = 0\n",
    "output[\"0\"] = 0\n",
    "output[\"1 words\"] = \"\"\n",
    "output[\"0 words\"] = \"\"\n",
    "output[\"1 assistance\"] = 0\n",
    "output[\"0 assistance\"] = 0\n",
    "output[\"1 word count\"] = 0\n",
    "output[\"0 word count\"] = 0\n",
    "output[\"1 list\"] = \"\"\n",
    "output[\"0 list\"] = \"\"\n",
    "\n",
    "skip_email = False\n",
    "\n",
    "#go through line-by-line and count assistance\n",
    "#note any new words in spam / not spam\n",
    "#and count spam emails\n",
    "\n",
    "for line in input:\n",
    "\n",
    "    skip_email = False\n",
    "\n",
    "    if len(line.split('\\t')) == 4:\n",
    "        tmp = line.split('\\t')\n",
    "        all_text = tmp[2] + \" \" + tmp[3]\n",
    "        true_label = tmp[1]\n",
    "\n",
    "        #empty subject\n",
    "    elif len(line.split('\\t')) == 3:\n",
    "        tmp = line.split('\\t')\n",
    "        all_text = tmp[-1]\n",
    "        true_label = tmp[1]\n",
    "\n",
    "    else:\n",
    "        skip_email = True\n",
    "\n",
    "    clean_line = re.split(\"[, \\.\\n]+\", all_text)\n",
    "\n",
    "    if skip_email is False:\n",
    "        #count number of times assistance is present\n",
    "        #output[\"assistance\"] += len(re.findall(\"assistance\",clean_line))\n",
    "\n",
    "        #append the email text\n",
    "        id = tmp[0]\n",
    "        output[id] = all_text\n",
    "\n",
    "        #if spam\n",
    "        if tmp[1] == \"1\":\n",
    "            #add id to spam list\n",
    "            output[\"1 list\"] += id + \" \"\n",
    "\n",
    "            #check to see if we've seen the word before in spam\n",
    "            for word in re.split(\"[, \\.\\n]+\", all_text):\n",
    "                if word not in output[\"1 words\"].split(' '):\n",
    "                    output[\"1 words\"] = output[\"1 words\"] + word + \" \"\n",
    "\n",
    "            #increment spam count, assistance in spam count, and words in spam\n",
    "            matches = re.findall(r'\\bassistance\\b',all_text)\n",
    "\n",
    "            output[\"1 assistance\"] += len(matches)\n",
    "            output[\"1 word count\"] += len(clean_line)\n",
    "            output[\"1\"] += 1\n",
    "\n",
    "                #if len(matches) > 0:\n",
    "                #       print line\n",
    "        #not spam\n",
    "        else:\n",
    "            #add id to not spam list\n",
    "            output[\"0 list\"] += id + \" \"\n",
    "\n",
    "            #check to see if we've seen the word before in not spam\n",
    "            for word in re.split(\"[, \\.\\n]+\", all_text):\n",
    "                    if word not in output[\"0 words\"].split(' '):\n",
    "                            output[\"0 words\"] = output[\"0 words\"] + word + \" \"\n",
    "\n",
    "            #increment not spam count, assistance in not spam count, and words\n",
    "            #not in spam\n",
    "            matches = re.findall(r'\\bassistance\\b',all_text)\n",
    "            output[\"0 assistance\"] += len(matches)\n",
    "            output[\"0 word count\"] += len(clean_line)\n",
    "            output[\"0\"] += 1\n",
    "\n",
    "                        #if len(matches) >0:\n",
    "                        #       print line\n",
    "#print all info\n",
    "for key, value in output.items():\n",
    "    print key + \"\\t\" + str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nb_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nb_reducer.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "output = {}\n",
    "output[\"1\"] = 0\n",
    "output[\"0\"] = 0\n",
    "output[\"1 words\"] = \"\"\n",
    "output[\"0 words\"] = \"\"\n",
    "output[\"1 assistance\"] = 0\n",
    "output[\"0 assistance\"] = 0\n",
    "output[\"1 word count\"] = 0\n",
    "output[\"0 word count\"] = 0\n",
    "output[\"1 list\"] = \"\"\n",
    "output[\"0 list\"] = \"\"\n",
    "\n",
    "input = []\n",
    "\n",
    "#take all inputs and then sort for aggregation\n",
    "for line in sys.stdin:\n",
    "\n",
    "    if line != \"\\n\":\n",
    "        lines = input.append(line)\n",
    "\n",
    "#merge into dictionary\n",
    "for line in input:\n",
    "    tmp = line.split('\\t')\n",
    "\n",
    "    if len(tmp) == 2:\n",
    "        cur_key = tmp[0]\n",
    "        cur_value = tmp[1]\n",
    "\n",
    "        #if a numerical field\n",
    "        if (cur_key == \"1 word count\") or (cur_key == \"0 word count\") or (cur_key == \"0\") or (cur_key == \"1\") or (cur_key == \"1 assistance\") or (cur_key == \"0 assistance\"):\n",
    "            output[cur_key] += int(cur_value)\n",
    "\n",
    "        #if a string list\n",
    "        elif (cur_key == \"1 words\") or (cur_key == \"0 words\") or (cur_key == \"1 list\") or (cur_key == \"0 list\"):\n",
    "#                       print cur_key\n",
    "            output[cur_key] += cur_value\n",
    "\n",
    "        #user email content\n",
    "        else:\n",
    "            output[cur_key] = cur_value\n",
    "\n",
    "tmp_str = \"\"\n",
    "tmp_list = []\n",
    "\n",
    "#dedup unique words\n",
    "for word in output[\"0 words\"].split(' '):\n",
    "    if word not in tmp_list:\n",
    "        tmp_str += word + \" \"\n",
    "\n",
    "output[\"0 words\"] = tmp_str\n",
    "\n",
    "tmp_str = \"\"\n",
    "tmp_list = []\n",
    "\n",
    "for word in output[\"1 words\"].split(' '):\n",
    "    if word not in tmp_list:\n",
    "        tmp_str += word + \" \"\n",
    "\n",
    "output[\"1 words\"] = tmp_str\n",
    "\n",
    "#for key, value in output.items():\n",
    "#       print key + \",\" + str(value)\n",
    "\n",
    "#i reused old code to save time since this was a longgggg assignment, hence the strange switch in variables\n",
    "#extract the data in each chunk\n",
    "spam_count = output[\"1\"]\n",
    "email_count = output[\"1\"] + output[\"0\"]\n",
    "words_in_spam = output[\"1 word count\"]\n",
    "words_in_not_spam = output[\"0 word count\"]\n",
    "target_in_spam = output[\"1 assistance\"]\n",
    "target_in_not_spam = output[\"0 assistance\"]\n",
    "set_of_spam = output[\"1 words\"]\n",
    "set_of_not_spam = output[\"0 words\"]\n",
    "\n",
    "#going to total count of words in each class instead of unique\n",
    "total_words_in_spam = len(set_of_spam.split(' '))\n",
    "total_words_in_not_spam = len(set_of_not_spam.split(' '))\n",
    "\n",
    "#print total_words_in_spam\n",
    "#print total_words_in_not_spam\n",
    "\n",
    "p_spam = float(spam_count) / float(email_count)\n",
    "p_not_spam = 1 - p_spam\n",
    "\n",
    "error_count = 0\n",
    "\n",
    "#compute probability of spam vs not spam\n",
    "for key, value in output.items():\n",
    "\n",
    "    #if an email\n",
    "    if (key != \"1 words\") and (key != \"0 words\") and (key != \"1\") and (key != \"0\") and (key != \"1 word count\") and (key != \"0 word count\") and (key != \"1 assistance\") and (key != \"0 word count\") and (key != \"1 list\") and (key != \"0 list\"):\n",
    "\n",
    "        assistance_count = len(re.findall('\\bassistance\\b',str(value)))\n",
    "        ID = key\n",
    "\n",
    "        if ID in output[\"1 list\"].split(' '):\n",
    "            true_label = \"1\"\n",
    "        else:\n",
    "            true_label = \"0\"\n",
    "\n",
    "        p_cond_spam = float(target_in_spam) / float(total_words_in_spam)\n",
    "        p_cond_not_spam = float(target_in_not_spam) / float(total_words_in_not_spam)\n",
    "\n",
    "        #P(SPAM | word) = p_spam * P(word|SPAM) * word_count\n",
    "\n",
    "        log_prob_spam = math.log(p_spam) + math.log(p_cond_spam) * float(assistance_count)\n",
    "        log_prob_not_spam = math.log(p_not_spam) + math.log(p_cond_not_spam) * float(assistance_count)\n",
    "        \n",
    "        #if we predict spam\n",
    "        if log_prob_spam > log_prob_not_spam:\n",
    "            print ID + \",1,\" + true_label + \",\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "        \n",
    "            if true_label is not \"1\":\n",
    "                error_count += 1\n",
    "        \n",
    "        #if we predict not spam\n",
    "        else:\n",
    "            print ID + \",0,\" + true_label + \",\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "\n",
    "            if true_label is not \"0\":\n",
    "                error_count += 1\n",
    "\n",
    "print float(error_count) / float(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HW23.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW23.sh\n",
    "\n",
    "hdfs dfs -rm /tmp/enronemail_1h.txt\n",
    "hdfs dfs -put enronemail_1h.txt /tmp/enronemail_1h.txt\n",
    "hdfs dfs -rm -r /tmp/output_HW23\n",
    "hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.5.jar -file ~/test/nb_mapper.py ~/test/nb_reducer.py -mapper \"python nb_mapper.py\" -reducer \"python nb_reducer.py\" -input /tmp/enronemail_1h.txt -output /tmp/output_HW23\n",
    "hdfs dfs -cat /tmp/output_HW23/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x nb_mapper.py; chmod +x nb_reducer.py; chmod +x HW23.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0010.2001-06-28.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0001.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0018.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.1999-12-12.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0001.1999-12-10.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0011.2001-06-29.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0008.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0009.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0017.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0011.2001-06-28.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0015.2001-07-05.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0015.2001-02-12.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0009.2001-06-26.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0017.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0012.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0003.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0004.2001-06-12.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0008.2001-06-12.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0007.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0016.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0015.2000-06-09.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0016.1999-12-15.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0013.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0005.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0012.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0016.2001-07-06.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0011.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0009.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0006.2001-02-08.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0015.1999-12-15.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0014.2003-12-19.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0010.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0010.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0014.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0006.1999-12-13.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0003.2001-02-08.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0001.2001-02-07.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0008.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0007.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0017.2004-08-02.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0014.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0006.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0016.2001-07-05.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0008.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0014.2001-07-04.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0001.2001-04-02.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0012.2000-06-08.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0014.1999-12-15.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0009.2000-06-07.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0008.2001-06-25.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0017.2001-04-03.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0014.2001-02-12.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0 assistance,0,0,-0.82098055207,-0.579818495253\r\n",
      "0004.1999-12-10.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0001.2000-06-06.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0011.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0004.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0018.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0007.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0016.2003-12-19.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0004.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0015.2003-12-19.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0006.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0009.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0002.1999-12-13.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.2000-06-06.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0010.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0007.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0003.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0003.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0017.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0013.2001-06-30.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0003.1999-12-10.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0012.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0009.1999-12-13.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0018.2001-07-13.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0002.2001-02-07.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0007.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0012.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.2001-06-23.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0013.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0007.1999-12-13.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0017.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0006.2001-06-25.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0006.2001-04-03.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.2001-02-08.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0002.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0003.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0013.2001-04-03.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0004.2001-04-02.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0010.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0001.1999-12-10.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0013.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0015.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0012.2003-12-19.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0016.2001-02-12.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0002.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0002.2001-05-25.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0011.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0.44\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | python nb_mapper.py | python nb_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.**\n",
    "\n",
    "**For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "**OR the original paper by the curators of the Enron email data:**\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf\n",
    "\n",
    "**p(term|Spam) = ( term_count(spam only) + 1 ) / ( total_word_count(spam only) + num_unique_words(spam only) ) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nb_mapper_laplace.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nb_mapper_laplace.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "\n",
    "input = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    input.append(line)\n",
    "        #matches = re.findall('\\bassistance\\b',line)\n",
    "        #if len(matches) > 0:\n",
    "        #       print matches\n",
    "\n",
    "output = {}\n",
    "output[\"1\"] = 0\n",
    "output[\"0\"] = 0\n",
    "output[\"1 words\"] = \"\"\n",
    "output[\"0 words\"] = \"\"\n",
    "output[\"1 assistance\"] = 0\n",
    "output[\"0 assistance\"] = 0\n",
    "output[\"1 word count\"] = 0\n",
    "output[\"0 word count\"] = 0\n",
    "output[\"1 list\"] = \"\"\n",
    "output[\"0 list\"] = \"\"\n",
    "\n",
    "skip_email = False\n",
    "\n",
    "#go through line-by-line and count assistance\n",
    "#note any new words in spam / not spam\n",
    "#and count spam emails\n",
    "\n",
    "for line in input:\n",
    "\n",
    "    skip_email = False\n",
    "\n",
    "    if len(line.split('\\t')) == 4:\n",
    "        tmp = line.split('\\t')\n",
    "        all_text = tmp[2] + \" \" + tmp[3]\n",
    "        true_label = tmp[1]\n",
    "\n",
    "        #empty subject\n",
    "    elif len(line.split('\\t')) == 3:\n",
    "        tmp = line.split('\\t')\n",
    "        all_text = tmp[-1]\n",
    "        true_label = tmp[1]\n",
    "\n",
    "    else:\n",
    "        skip_email = True\n",
    "\n",
    "    clean_line = re.split(\"[, \\.\\n]+\", all_text)\n",
    "\n",
    "    if skip_email is False:\n",
    "        #count number of times assistance is present\n",
    "        #output[\"assistance\"] += len(re.findall(\"assistance\",clean_line))\n",
    "\n",
    "        #append the email text\n",
    "        id = tmp[0]\n",
    "        output[id] = all_text\n",
    "\n",
    "        #if spam\n",
    "        if tmp[1] == \"1\":\n",
    "            #add id to spam list\n",
    "            output[\"1 list\"] += id + \" \"\n",
    "\n",
    "            #check to see if we've seen the word before in spam\n",
    "            for word in re.split(\"[, \\.\\n]+\", all_text):\n",
    "                if word not in output[\"1 words\"].split(' '):\n",
    "                    output[\"1 words\"] = output[\"1 words\"] + word + \" \"\n",
    "\n",
    "            #increment spam count, assistance in spam count, and words in spam\n",
    "            matches = re.findall(r'\\bassistance\\b',all_text)\n",
    "\n",
    "            output[\"1 assistance\"] += len(matches)\n",
    "            output[\"1 word count\"] += len(clean_line)\n",
    "            output[\"1\"] += 1\n",
    "\n",
    "                #if len(matches) > 0:\n",
    "                #       print line\n",
    "        #not spam\n",
    "        else:\n",
    "            #add id to not spam list\n",
    "            output[\"0 list\"] += id + \" \"\n",
    "\n",
    "            #check to see if we've seen the word before in not spam\n",
    "            for word in re.split(\"[, \\.\\n]+\", all_text):\n",
    "                    if word not in output[\"0 words\"].split(' '):\n",
    "                            output[\"0 words\"] = output[\"0 words\"] + word + \" \"\n",
    "\n",
    "            #increment not spam count, assistance in not spam count, and words\n",
    "            #not in spam\n",
    "            matches = re.findall(r'\\bassistance\\b',all_text)\n",
    "            output[\"0 assistance\"] += len(matches)\n",
    "            output[\"0 word count\"] += len(clean_line)\n",
    "            output[\"0\"] += 1\n",
    "\n",
    "                        #if len(matches) >0:\n",
    "                        #       print line\n",
    "#print all info\n",
    "for key, value in output.items():\n",
    "    print key + \"\\t\" + str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nb_reducer_laplace.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nb_reducer_laplace.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "output = {}\n",
    "output[\"1\"] = 0\n",
    "output[\"0\"] = 0\n",
    "output[\"1 words\"] = \"\"\n",
    "output[\"0 words\"] = \"\"\n",
    "output[\"1 assistance\"] = 0\n",
    "output[\"0 assistance\"] = 0\n",
    "output[\"1 word count\"] = 0\n",
    "output[\"0 word count\"] = 0\n",
    "output[\"1 list\"] = \"\"\n",
    "output[\"0 list\"] = \"\"\n",
    "\n",
    "input = []\n",
    "\n",
    "#take all inputs and then sort for aggregation\n",
    "for line in sys.stdin:\n",
    "\n",
    "    if line != \"\\n\":\n",
    "        lines = input.append(line)\n",
    "\n",
    "#merge into dictionary\n",
    "for line in input:\n",
    "    tmp = line.split('\\t')\n",
    "\n",
    "    if len(tmp) == 2:\n",
    "        cur_key = tmp[0]\n",
    "        cur_value = tmp[1]\n",
    "\n",
    "        #if a numerical field\n",
    "        if (cur_key == \"1 word count\") or (cur_key == \"0 word count\") or (cur_key == \"0\") or (cur_key == \"1\") or (cur_key == \"1 assistance\") or (cur_key == \"0 assistance\"):\n",
    "            output[cur_key] += int(cur_value)\n",
    "\n",
    "        #if a string list\n",
    "        elif (cur_key == \"1 words\") or (cur_key == \"0 words\") or (cur_key == \"1 list\") or (cur_key == \"0 list\"):\n",
    "#                       print cur_key\n",
    "            output[cur_key] += cur_value\n",
    "\n",
    "        #user email content\n",
    "        else:\n",
    "            output[cur_key] = cur_value\n",
    "\n",
    "tmp_str = \"\"\n",
    "tmp_list = []\n",
    "\n",
    "#dedup unique words\n",
    "for word in output[\"0 words\"].split(' '):\n",
    "    if word not in tmp_list:\n",
    "        tmp_str += word + \" \"\n",
    "\n",
    "output[\"0 words\"] = tmp_str\n",
    "\n",
    "tmp_str = \"\"\n",
    "tmp_list = []\n",
    "\n",
    "for word in output[\"1 words\"].split(' '):\n",
    "    if word not in tmp_list:\n",
    "        tmp_str += word + \" \"\n",
    "\n",
    "output[\"1 words\"] = tmp_str\n",
    "\n",
    "#for key, value in output.items():\n",
    "#       print key + \",\" + str(value)\n",
    "\n",
    "#i reused old code to save time since this was a longgggg assignment, hence the strange switch in variables\n",
    "#extract the data in each chunk\n",
    "spam_count = output[\"1\"]\n",
    "email_count = output[\"1\"] + output[\"0\"]\n",
    "words_in_spam = output[\"1 word count\"]\n",
    "words_in_not_spam = output[\"0 word count\"]\n",
    "target_in_spam = output[\"1 assistance\"]\n",
    "target_in_not_spam = output[\"0 assistance\"]\n",
    "set_of_spam = output[\"1 words\"]\n",
    "set_of_not_spam = output[\"0 words\"]\n",
    "\n",
    "#going to total count of words in each class instead of unique\n",
    "total_words_in_spam = len(set_of_spam.split(' '))\n",
    "total_words_in_not_spam = len(set_of_not_spam.split(' '))\n",
    "\n",
    "#print total_words_in_spam\n",
    "#print total_words_in_not_spam\n",
    "\n",
    "p_spam = float(spam_count) / float(email_count)\n",
    "p_not_spam = 1 - p_spam\n",
    "\n",
    "error_count = 0\n",
    "\n",
    "#compute probability of spam vs not spam\n",
    "for key, value in output.items():\n",
    "\n",
    "    #if an email\n",
    "    if (key != \"1 words\") and (key != \"0 words\") and (key != \"1\") and (key != \"0\") and (key != \"1 word count\") and (key != \"0 word count\") and (key != \"1 assistance\") and (key != \"0 word count\") and (key != \"1 list\") and (key != \"0 list\"):\n",
    "\n",
    "        assistance_count = len(re.findall('\\bassistance\\b',str(value)))\n",
    "        ID = key\n",
    "\n",
    "        if ID in output[\"1 list\"].split(' '):\n",
    "            true_label = \"1\"\n",
    "        else:\n",
    "            true_label = \"0\"\n",
    "\n",
    "        #p(term|Spam) = ( term_count(spam only) + 1 ) / ( total_word_count(spam only) + num_unique_words(spam only) ) \n",
    "        #for laplace, not quite sure I understand the equation and I don't have time to figure it out.\n",
    "        #so, I just added one to the numerator and kept the denominator the same. It's the same idea\n",
    "        #and the equation is engineered anyway since we aren't even dividing by P(term)\n",
    "        \n",
    "        p_cond_spam = float(target_in_spam + 1) / float(total_words_in_spam)\n",
    "        p_cond_not_spam = float(target_in_not_spam + 1) / float(total_words_in_not_spam)\n",
    "\n",
    "        #P(SPAM | word) = p_spam * P(word|SPAM) * word_count\n",
    "\n",
    "        log_prob_spam = math.log(p_spam) + math.log(p_cond_spam) * float(assistance_count)\n",
    "        log_prob_not_spam = math.log(p_not_spam) + math.log(p_cond_not_spam) * float(assistance_count)\n",
    "        \n",
    "        #if we predict spam\n",
    "        if log_prob_spam > log_prob_not_spam:\n",
    "            print ID + \",1,\" + true_label + \",\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "        \n",
    "            if true_label is not \"1\":\n",
    "                error_count += 1\n",
    "        \n",
    "        #if we predict not spam\n",
    "        else:\n",
    "            print ID + \",0,\" + true_label + \",\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "\n",
    "            if true_label is not \"0\":\n",
    "                error_count += 1\n",
    "\n",
    "print float(error_count) / float(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HW24.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW24.sh\n",
    "\n",
    "hdfs dfs -rm /tmp/enronemail_1h.txt\n",
    "hdfs dfs -put enronemail_1h.txt /tmp/enronemail_1h.txt\n",
    "hdfs dfs -rm -r /tmp/output_HW24\n",
    "hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.5.jar -file ~/test/nb_mapper_laplace.py ~/test/nb_reducer_laplace.py -mapper \"python nb_mapper_laplace.py\" -reducer \"python nb_reducer_laplace.py\" -input /tmp/enronemail_1h.txt -output /tmp/output_HW24\n",
    "hdfs dfs -cat /tmp/output_HW24/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x nb_mapper_laplace.py; chmod +x nb_reducer_laplace.py; chmod +x HW24.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0010.2001-06-28.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0001.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0018.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.1999-12-12.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0001.1999-12-10.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0011.2001-06-29.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0008.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0009.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0017.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0011.2001-06-28.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0015.2001-07-05.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0015.2001-02-12.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0009.2001-06-26.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0017.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0012.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0003.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0004.2001-06-12.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0008.2001-06-12.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0007.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0016.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0015.2000-06-09.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0016.1999-12-15.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0013.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0005.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0012.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0016.2001-07-06.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0011.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0009.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0006.2001-02-08.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0015.1999-12-15.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0014.2003-12-19.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0010.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0010.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0014.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0006.1999-12-13.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0003.2001-02-08.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0001.2001-02-07.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0008.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0007.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0017.2004-08-02.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0014.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0006.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0016.2001-07-05.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0008.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0014.2001-07-04.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0001.2001-04-02.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0012.2000-06-08.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0014.1999-12-15.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0009.2000-06-07.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0008.2001-06-25.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0017.2001-04-03.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0014.2001-02-12.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0 assistance,0,0,-0.82098055207,-0.579818495253\r\n",
      "0004.1999-12-10.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0001.2000-06-06.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0011.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0004.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0018.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0007.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0016.2003-12-19.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0004.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0015.2003-12-19.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0006.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0009.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0002.1999-12-13.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.2000-06-06.lokay,0,0,-0.82098055207,-0.579818495253\r\n",
      "0010.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0007.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0003.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0003.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0017.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0013.2001-06-30.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0003.1999-12-10.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0012.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0009.1999-12-13.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0018.2001-07-13.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0002.2001-02-07.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0007.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0012.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.2001-06-23.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0013.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0007.1999-12-13.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0017.2000-01-17.beck,0,0,-0.82098055207,-0.579818495253\r\n",
      "0006.2001-06-25.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0006.2001-04-03.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0005.2001-02-08.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0002.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0003.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0013.2001-04-03.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0004.2001-04-02.williams,0,0,-0.82098055207,-0.579818495253\r\n",
      "0010.2001-02-09.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0001.1999-12-10.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0013.1999-12-14.farmer,0,0,-0.82098055207,-0.579818495253\r\n",
      "0015.1999-12-14.kaminski,0,0,-0.82098055207,-0.579818495253\r\n",
      "0012.2003-12-19.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0016.2001-02-12.kitchen,0,0,-0.82098055207,-0.579818495253\r\n",
      "0002.2004-08-01.BG,0,1,-0.82098055207,-0.579818495253\r\n",
      "0002.2001-05-25.SA_and_HP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0011.2003-12-18.GP,0,1,-0.82098055207,-0.579818495253\r\n",
      "0.44\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | python nb_mapper_laplace.py | python nb_reducer_laplace.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My error rates are the same with laplace smoothing, which may be due to an error on my part with the laplace smoothing denominator.  Also, we're using only \"assistance\" which exists, so the laplace smoothing might not do much.  It is meant to really prevent log(0) computation for words that do not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nb_mapper_laplace_over3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nb_mapper_laplace_over3.py\n",
    "\n",
    "#the mapper is changed to keep count of all words seen in the dictionary\n",
    "#so the reducer can filter and compute\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "\n",
    "input = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    input.append(line)\n",
    "        #matches = re.findall('\\bassistance\\b',line)\n",
    "        #if len(matches) > 0:\n",
    "        #       print matches\n",
    "\n",
    "output = {}\n",
    "output[\"1\"] = 0\n",
    "output[\"0\"] = 0\n",
    "output[\"1 words\"] = \"\"\n",
    "output[\"0 words\"] = \"\"\n",
    "output[\"1 assistance\"] = 0\n",
    "output[\"0 assistance\"] = 0\n",
    "output[\"1 word count\"] = 0\n",
    "output[\"0 word count\"] = 0\n",
    "output[\"1 list\"] = \"\"\n",
    "output[\"0 list\"] = \"\"\n",
    "\n",
    "skip_email = False\n",
    "\n",
    "#go through line-by-line and count assistance\n",
    "#note any new words in spam / not spam\n",
    "#and count spam emails\n",
    "\n",
    "for line in input:\n",
    "\n",
    "    skip_email = False\n",
    "\n",
    "    if len(line.split('\\t')) == 4:\n",
    "        tmp = line.split('\\t')\n",
    "        all_text = tmp[2] + \" \" + tmp[3]\n",
    "        true_label = tmp[1]\n",
    "\n",
    "        #empty subject\n",
    "    elif len(line.split('\\t')) == 3:\n",
    "        tmp = line.split('\\t')\n",
    "        all_text = tmp[-1]\n",
    "        true_label = tmp[1]\n",
    "\n",
    "    else:\n",
    "        skip_email = True\n",
    "\n",
    "    clean_line = re.split(\"[, \\.\\n]+\", all_text)\n",
    "\n",
    "    if skip_email is False:\n",
    "        #count number of times assistance is present\n",
    "        #output[\"assistance\"] += len(re.findall(\"assistance\",clean_line))\n",
    "\n",
    "        #append the email text\n",
    "        id = tmp[0]\n",
    "        output[id] = all_text\n",
    "        \n",
    "        #add words seen and increment keys\n",
    "        tokens = re.split(\"[, \\.\\n]+\", all_text)\n",
    "        for word in tokens:\n",
    "            if word in output.keys():\n",
    "                output[word] += 1\n",
    "            else:\n",
    "                output[word] = 1\n",
    "        \n",
    "\n",
    "        #if spam\n",
    "        if tmp[1] == \"1\":\n",
    "            #add id to spam list\n",
    "            output[\"1 list\"] += id + \" \"\n",
    "\n",
    "            #check to see if we've seen the word before in spam\n",
    "            for word in re.split(\"[, \\.\\n]+\", all_text):\n",
    "                if word not in output[\"1 words\"].split(' '):\n",
    "                    output[\"1 words\"] = output[\"1 words\"] + word + \" \"\n",
    "\n",
    "            #increment spam count, assistance in spam count, and words in spam\n",
    "            matches = re.findall(r'\\bassistance\\b',all_text)\n",
    "\n",
    "            output[\"1 assistance\"] += len(matches)\n",
    "            output[\"1 word count\"] += len(clean_line)\n",
    "            output[\"1\"] += 1\n",
    "\n",
    "                #if len(matches) > 0:\n",
    "                #       print line\n",
    "        #not spam\n",
    "        else:\n",
    "            #add id to not spam list\n",
    "            output[\"0 list\"] += id + \" \"\n",
    "\n",
    "            #check to see if we've seen the word before in not spam\n",
    "            for word in re.split(\"[, \\.\\n]+\", all_text):\n",
    "                    if word not in output[\"0 words\"].split(' '):\n",
    "                            output[\"0 words\"] = output[\"0 words\"] + word + \" \"\n",
    "\n",
    "            #increment not spam count, assistance in not spam count, and words\n",
    "            #not in spam\n",
    "            matches = re.findall(r'\\bassistance\\b',all_text)\n",
    "            output[\"0 assistance\"] += len(matches)\n",
    "            output[\"0 word count\"] += len(clean_line)\n",
    "            output[\"0\"] += 1\n",
    "\n",
    "                        #if len(matches) >0:\n",
    "                        #       print line\n",
    "#print all info\n",
    "for key, value in output.items():\n",
    "    print key + \"\\t\" + str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nb_reducer_laplace_over3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nb_reducer_laplace_over3.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "output = {}\n",
    "output[\"1\"] = 0\n",
    "output[\"0\"] = 0\n",
    "output[\"1 words\"] = \"\"\n",
    "output[\"0 words\"] = \"\"\n",
    "output[\"1 assistance\"] = 0\n",
    "output[\"0 assistance\"] = 0\n",
    "output[\"1 word count\"] = 0\n",
    "output[\"0 word count\"] = 0\n",
    "output[\"1 list\"] = \"\"\n",
    "output[\"0 list\"] = \"\"\n",
    "\n",
    "input = []\n",
    "\n",
    "word_freq = {}\n",
    "\n",
    "#take all inputs and then sort for aggregation\n",
    "for line in sys.stdin:\n",
    "\n",
    "    if line != \"\\n\":\n",
    "        lines = input.append(line)\n",
    "\n",
    "#merge stats and lists into dictionary\n",
    "for line in input:\n",
    "    tmp = line.split('\\t')\n",
    "\n",
    "    if len(tmp) == 2:\n",
    "        cur_key = tmp[0]\n",
    "        cur_value = tmp[1]\n",
    "\n",
    "        #if a numerical field\n",
    "        if (cur_key == \"1 word count\") or (cur_key == \"0 word count\") or (cur_key == \"0\") or (cur_key == \"1\") or (cur_key == \"1 assistance\") or (cur_key == \"0 assistance\"):\n",
    "            output[cur_key] += int(cur_value)\n",
    "\n",
    "        #if a string list\n",
    "        elif (cur_key == \"1 words\") or (cur_key == \"0 words\") or (cur_key == \"1 list\") or (cur_key == \"0 list\"):\n",
    "#                       print cur_key\n",
    "            output[cur_key] += cur_value\n",
    "\n",
    "#merge word counts (depends on complete lists hence the separation\n",
    "for line in input:\n",
    "    tmp = line.split('\\t')\n",
    "\n",
    "    if len(tmp) == 2:\n",
    "        cur_key = tmp[0]\n",
    "        cur_value = tmp[1]\n",
    "        \n",
    "        spam_words = output[\"1 words\"]\n",
    "        not_spam_words = output[\"0 words\"]\n",
    "        #if key is a word in either spam or not spam\n",
    "        if (len(re.findall('\\b'+cur_key+'\\b',spam_words)) > 0) or (len(re.findall('\\b'+cur_key+'\\b',not_spam_words)) > 0):\n",
    "                #check frequency, if under 3 del\n",
    "                if int(output[cur_key]) < 3:\n",
    "                    del output[cur_key]\n",
    "            \n",
    "print output\n",
    "        \n",
    "#recompute the stats previously done\n",
    "\n",
    "tmp_str = \"\"\n",
    "tmp_list = []\n",
    "\n",
    "#dedup unique words\n",
    "for word in output[\"0 words\"].split(' '):\n",
    "    if word not in tmp_list:\n",
    "        tmp_str += word + \" \"\n",
    "\n",
    "output[\"0 words\"] = tmp_str\n",
    "\n",
    "tmp_str = \"\"\n",
    "tmp_list = []\n",
    "\n",
    "for word in output[\"1 words\"].split(' '):\n",
    "    if word not in tmp_list:\n",
    "        tmp_str += word + \" \"\n",
    "\n",
    "output[\"1 words\"] = tmp_str\n",
    "\n",
    "#for key, value in output.items():\n",
    "#       print key + \",\" + str(value)\n",
    "\n",
    "#i reused old code to save time since this was a longgggg assignment, hence the strange switch in variables\n",
    "#extract the data in each chunk\n",
    "spam_count = output[\"1\"]\n",
    "email_count = output[\"1\"] + output[\"0\"]\n",
    "words_in_spam = output[\"1 word count\"]\n",
    "words_in_not_spam = output[\"0 word count\"]\n",
    "target_in_spam = output[\"1 assistance\"]\n",
    "target_in_not_spam = output[\"0 assistance\"]\n",
    "set_of_spam = output[\"1 words\"]\n",
    "set_of_not_spam = output[\"0 words\"]\n",
    "\n",
    "#going to total count of words in each class instead of unique\n",
    "total_words_in_spam = len(set_of_spam.split(' '))\n",
    "total_words_in_not_spam = len(set_of_not_spam.split(' '))\n",
    "\n",
    "#print total_words_in_spam\n",
    "#print total_words_in_not_spam\n",
    "\n",
    "p_spam = float(spam_count) / float(email_count)\n",
    "p_not_spam = 1 - p_spam\n",
    "\n",
    "error_count = 0\n",
    "\n",
    "#compute probability of spam vs not spam\n",
    "for key, value in output.items():\n",
    "\n",
    "    #if an email\n",
    "    if (key != \"1 words\") and (key != \"0 words\") and (key != \"1\") and (key != \"0\") and (key != \"1 word count\") and (key != \"0 word count\") and (key != \"1 assistance\") and (key != \"0 word count\") and (key != \"1 list\") and (key != \"0 list\"):\n",
    "\n",
    "        assistance_count = len(re.findall('\\bassistance\\b',str(value)))\n",
    "        ID = key\n",
    "\n",
    "        if ID in output[\"1 list\"].split(' '):\n",
    "            true_label = \"1\"\n",
    "        else:\n",
    "            true_label = \"0\"\n",
    "\n",
    "        #p(term|Spam) = ( term_count(spam only) + 1 ) / ( total_word_count(spam only) + num_unique_words(spam only) ) \n",
    "        #for laplace, not quite sure I understand the equation and I don't have time to figure it out.\n",
    "        #so, I just added one to the numerator and kept the denominator the same. It's the same idea\n",
    "        #and the equation is engineered anyway since we aren't even dividing by P(term)\n",
    "        \n",
    "        p_cond_spam = float(target_in_spam + 1) / float(total_words_in_spam)\n",
    "        p_cond_not_spam = float(target_in_not_spam + 1) / float(total_words_in_not_spam)\n",
    "\n",
    "        #P(SPAM | word) = p_spam * P(word|SPAM) * word_count\n",
    "\n",
    "        log_prob_spam = math.log(p_spam) + math.log(p_cond_spam) * float(assistance_count)\n",
    "        log_prob_not_spam = math.log(p_not_spam) + math.log(p_cond_not_spam) * float(assistance_count)\n",
    "        \n",
    "        #if we predict spam\n",
    "        if log_prob_spam > log_prob_not_spam:\n",
    "            print ID + \",1,\" + true_label + \",\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "        \n",
    "            if true_label is not \"1\":\n",
    "                error_count += 1\n",
    "        \n",
    "        #if we predict not spam\n",
    "        else:\n",
    "            print ID + \",0,\" + true_label + \",\" + str(log_prob_spam) + \",\" + str(log_prob_not_spam)\n",
    "\n",
    "            if true_label is not \"0\":\n",
    "                error_count += 1\n",
    "\n",
    "print float(error_count) / float(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HW25.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW25.sh\n",
    "\n",
    "hdfs dfs -rm /tmp/enronemail_1h.txt\n",
    "hdfs dfs -put enronemail_1h.txt /tmp/enronemail_1h.txt\n",
    "hdfs dfs -rm -r /tmp/output_HW25\n",
    "hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.5.jar -file ~/test/nb_mapper_laplace_over3.py ~/test/nb_reducer_laplace_over3.py -mapper \"python nb_mapper_laplace_over3.py\" -reducer \"python nb_reducer_laplace_over3.py\" -input /tmp/enronemail_1h.txt -output /tmp/output_HW25\n",
    "hdfs dfs -cat /tmp/output_HW25/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x nb_mapper_laplace_over3.py; chmod +x nb_reducer_laplace_over3.py; chmod +x HW25.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"nb_reducer_laplace_over3.py\", line 56, in <module>\r\n",
      "    if (len(re.findall('\\b'+cur_key+'\\b',spam_words)) > 0) or (len(re.findall('\\b'+cur_key+'\\b',not_spam_words) > 0)):\r\n",
      "TypeError: object of type 'bool' has no len()\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | python nb_mapper_laplace_over3.py | python nb_reducer_laplace_over3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
